{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T13:58:21.743587Z","iopub.execute_input":"2023-09-15T13:58:21.743988Z","iopub.status.idle":"2023-09-15T13:58:21.754080Z","shell.execute_reply.started":"2023-09-15T13:58:21.743958Z","shell.execute_reply":"2023-09-15T13:58:21.752649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"About this Competition\n\nWhere is this data from?\n\nThe collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43,943 tweets were collected. Each tweet is labelled as one of 4 classes, which are described below.\n\nClass Description\n\n2 News: the tweet links to factual news about climate change\n\n1 Pro: the tweet supports the belief of man-made climate change\n\n0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n\n-1 Anti: the tweet does not believe in man-made climate change Variable definitions\n\nFeatures\n\nsentiment: Which class a tweet belongs in (refer to Class Description above)\n\nmessage: Tweet body\n\ntweetid: Twitter unique id\n\nThe files provided\n\ntrain.csv - You will use this data to train your model.\n\ntest.csv - You will use this data to test your model.\n\nSampleSubmission.csv - is an example of what your submission file should look like. The order of the rows does not matter, but the names of the tweetid's must be correct.","metadata":{}},{"cell_type":"markdown","source":"Predict Overview: EA Twitter Sentiment Classification\n\nCompanies would like to determine how people perceive climate change and whether or not they believe it is a real threat. Our mission is to deliver a precise and durable solution to this objective, granting companies the ability to tap into a wide range of consumer sentiments across various demographics and geographic regions. This, in turn, enhances their understanding and empowers them to shape future marketing strategies based on valuable insights.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n%matplotlib inline\nimport nltk\nimport re\nimport string\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nnltk.download('vader_lexicon')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:21.756696Z","iopub.execute_input":"2023-09-15T13:58:21.757235Z","iopub.status.idle":"2023-09-15T13:58:21.840105Z","shell.execute_reply.started":"2023-09-15T13:58:21.757187Z","shell.execute_reply":"2023-09-15T13:58:21.838898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data set consists of two features and a label. The main feature is the message column that contains a tweet related to global warming. The label sentiment catagorizes tweets according to four classes, namely news, neutral, pro and anti. Our aim will be to create a machine learning model that will be able to acurately classify any tweet according to these four buckets based on the textual message data of a tween only.","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/edsa-sentiment-classification/test.csv')\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:21.842923Z","iopub.execute_input":"2023-09-15T13:58:21.843453Z","iopub.status.idle":"2023-09-15T13:58:21.890089Z","shell.execute_reply.started":"2023-09-15T13:58:21.843406Z","shell.execute_reply":"2023-09-15T13:58:21.888923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/edsa-sentiment-classification/train.csv')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:21.891855Z","iopub.execute_input":"2023-09-15T13:58:21.892227Z","iopub.status.idle":"2023-09-15T13:58:21.954925Z","shell.execute_reply.started":"2023-09-15T13:58:21.892196Z","shell.execute_reply":"2023-09-15T13:58:21.953581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploratory Data Analysis(EDA)","metadata":{}},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:21.958528Z","iopub.execute_input":"2023-09-15T13:58:21.959069Z","iopub.status.idle":"2023-09-15T13:58:21.967550Z","shell.execute_reply.started":"2023-09-15T13:58:21.959021Z","shell.execute_reply":"2023-09-15T13:58:21.966076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 15819 features and 3 columns","metadata":{}},{"cell_type":"code","source":"df_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:21.969196Z","iopub.execute_input":"2023-09-15T13:58:21.969678Z","iopub.status.idle":"2023-09-15T13:58:21.982471Z","shell.execute_reply.started":"2023-09-15T13:58:21.969634Z","shell.execute_reply":"2023-09-15T13:58:21.981549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 10546 features and 2 columns","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:21.983933Z","iopub.execute_input":"2023-09-15T13:58:21.984412Z","iopub.status.idle":"2023-09-15T13:58:21.999942Z","shell.execute_reply.started":"2023-09-15T13:58:21.984378Z","shell.execute_reply":"2023-09-15T13:58:21.998264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.001986Z","iopub.execute_input":"2023-09-15T13:58:22.003099Z","iopub.status.idle":"2023-09-15T13:58:22.014814Z","shell.execute_reply.started":"2023-09-15T13:58:22.003051Z","shell.execute_reply":"2023-09-15T13:58:22.013942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null values","metadata":{}},{"cell_type":"markdown","source":"The tweetid feature simply uniquely identifies each tweet and most probably will add no real value in classification machine model training.","metadata":{}},{"cell_type":"code","source":"df_train['tweetid'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.016119Z","iopub.execute_input":"2023-09-15T13:58:22.016715Z","iopub.status.idle":"2023-09-15T13:58:22.030170Z","shell.execute_reply.started":"2023-09-15T13:58:22.016681Z","shell.execute_reply":"2023-09-15T13:58:22.029306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop('tweetid', axis=1)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.033559Z","iopub.execute_input":"2023-09-15T13:58:22.034795Z","iopub.status.idle":"2023-09-15T13:58:22.049486Z","shell.execute_reply.started":"2023-09-15T13:58:22.034747Z","shell.execute_reply":"2023-09-15T13:58:22.048421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to analyze the length of tweets, we created a new feature called size which is a count of the number of characters per tweet.","metadata":{}},{"cell_type":"code","source":"size = [len(tweet) for tweet in df_train['message']]\ndf_train['size'] = size","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.050754Z","iopub.execute_input":"2023-09-15T13:58:22.051723Z","iopub.status.idle":"2023-09-15T13:58:22.075665Z","shell.execute_reply.started":"2023-09-15T13:58:22.051687Z","shell.execute_reply":"2023-09-15T13:58:22.074687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['size'].mode()[0]\ndf_train['size'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.076769Z","iopub.execute_input":"2023-09-15T13:58:22.077750Z","iopub.status.idle":"2023-09-15T13:58:22.094176Z","shell.execute_reply.started":"2023-09-15T13:58:22.077715Z","shell.execute_reply":"2023-09-15T13:58:22.093191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tweets range from 14 to 208 characters in length. The average size of a tweet is about 124 characters long. Most tweets are 140 characters in length.","metadata":{}},{"cell_type":"code","source":"plt.figure( figsize=(9,4))\nplt.hist(df_train['size'])\nplt.title(\"Distribution of Tweet Lengths\")\nplt.xlabel(\"Length of Tweet In Charaters\") #X-label of the data\nplt.ylabel(\"Number of Tweets\")      #Y_label of the data\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.095882Z","iopub.execute_input":"2023-09-15T13:58:22.096577Z","iopub.status.idle":"2023-09-15T13:58:22.436047Z","shell.execute_reply.started":"2023-09-15T13:58:22.096545Z","shell.execute_reply":"2023-09-15T13:58:22.434918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Box Plots\n\nBelow we attempt to visualize the 5 number summary of each category of tweet as well as the dataset as a whole using box and whiskers diagrams.","metadata":{}},{"cell_type":"code","source":"#creating class subsets for the datase\n\ndf_anti = df_train.copy()[df_train['sentiment'] == -1]\ndf_neutral = df_train.copy()[df_train['sentiment'] == 0]\ndf_pro = df_train.copy()[df_train['sentiment'] == 1]\ndf_news = df_train.copy()[df_train['sentiment'] == 2]\n\n#storing the size data in separate variables\n\npro_len = df_pro['size']\nneutral_len = df_neutral['size']\nanti_len = df_anti['size']\nnews_len = df_news['size']\ndata_len = df_train['size']\n\n#creating a list of all the length datasets\n\nlen_data = [pro_len, anti_len, neutral_len, news_len, data_len]\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(9,4))\n\n# Create the box plots\nax.boxplot(len_data, vert=False)\n\n# Set the labels for each box plot\nlabels = ['pro', 'anti', 'neutral', 'news', 'main data']\nax.set_yticklabels(labels)\n\n# Set the title and axis labels\nplt.title('Box and Whiskers Diagram For Tweet Lengths Per Category')\nplt.xlabel('Length In Characters')\nplt.ylabel('Category of Tweet')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.437854Z","iopub.execute_input":"2023-09-15T13:58:22.438608Z","iopub.status.idle":"2023-09-15T13:58:22.767328Z","shell.execute_reply.started":"2023-09-15T13:58:22.438561Z","shell.execute_reply":"2023-09-15T13:58:22.766102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning Data\nIn order to process the tweet messages more effectively the tweets are cleaned using the clean function defined in the code cell below. The clean function does the following.\n\n* Converts all tweet text to lowercase.\n* Removes URLs.\n* Removes punctuation.\n* Removes numbers.\n* Removes stopwords.\n* Removes line-break code syntax.","metadata":{}},{"cell_type":"code","source":"stopword=set(stopwords.words('english'))  \ndef clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.769171Z","iopub.execute_input":"2023-09-15T13:58:22.769938Z","iopub.status.idle":"2023-09-15T13:58:22.780442Z","shell.execute_reply.started":"2023-09-15T13:58:22.769892Z","shell.execute_reply":"2023-09-15T13:58:22.779107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"message\"] = df_train[\"message\"].apply(clean)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:22.781964Z","iopub.execute_input":"2023-09-15T13:58:22.782444Z","iopub.status.idle":"2023-09-15T13:58:23.777068Z","shell.execute_reply.started":"2023-09-15T13:58:22.782399Z","shell.execute_reply":"2023-09-15T13:58:23.775956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:23.778532Z","iopub.execute_input":"2023-09-15T13:58:23.778972Z","iopub.status.idle":"2023-09-15T13:58:23.791666Z","shell.execute_reply.started":"2023-09-15T13:58:23.778931Z","shell.execute_reply":"2023-09-15T13:58:23.790660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating Wordcloud To Analyse Commonly Used PhrasesÂ¶","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud,ImageColorGenerator\ntext = \" \".join(i for i in df_train[\"message\"])\ntext = str(text)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:23.793461Z","iopub.execute_input":"2023-09-15T13:58:23.794450Z","iopub.status.idle":"2023-09-15T13:58:23.810507Z","shell.execute_reply.started":"2023-09-15T13:58:23.794412Z","shell.execute_reply":"2023-09-15T13:58:23.809115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud()\ntweet_cloud = wordcloud.generate(text)\nplt.figure( figsize=(9,4))\nplt.imshow(tweet_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:23.812453Z","iopub.execute_input":"2023-09-15T13:58:23.812937Z","iopub.status.idle":"2023-09-15T13:58:26.457211Z","shell.execute_reply.started":"2023-09-15T13:58:23.812902Z","shell.execute_reply":"2023-09-15T13:58:26.456349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using a Word Cloud, we attempt to visualize which words are and phrases are most commonly used in tweets related to global warming. The top three phrases seem to be:\n\n* Climate Change\n* Global Warming\n* RT\n* Change RT\n* Believe Climate","metadata":{}},{"cell_type":"code","source":"has_rt = lambda x: 'rt' in x\ndf_train['rt'] = [1 if has_rt(i) else 0 for i in df_train['message']]\n#df_train['rt'] = df_train['message'].apply(has_rt)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:26.458664Z","iopub.execute_input":"2023-09-15T13:58:26.459423Z","iopub.status.idle":"2023-09-15T13:58:26.489103Z","shell.execute_reply.started":"2023-09-15T13:58:26.459388Z","shell.execute_reply":"2023-09-15T13:58:26.487681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RT (an abbreviation of 'retweet') could signal that many individuals either share the same sentiments surrounding climate change or are active in attempting to engage with others on the topic based on information shared by others.","metadata":{}},{"cell_type":"code","source":"rt_counts = df_train[\"rt\"].value_counts()\nrt_counts","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:26.490840Z","iopub.execute_input":"2023-09-15T13:58:26.491203Z","iopub.status.idle":"2023-09-15T13:58:26.499924Z","shell.execute_reply.started":"2023-09-15T13:58:26.491172Z","shell.execute_reply":"2023-09-15T13:58:26.498992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating a Pie Chart for Retweet Analysis\n\nMost tweets have an rt which means it is highly likely if not certain that they are Retweets.\n\nAccording to twitter.com:\n\n'A Retweet is a re-posting of a Tweet. Twitter's Retweet feature helps you and others quickly share that Tweet with all of your followers. You can Retweet your own Tweets or Tweets from someone else. Sometimes people type \"RT\" at the beginning of a Tweet to indicate that they are re-posting someone else's content. This isn't an official Twitter command or feature, but signifies that they are quoting another person's Tweet.'","metadata":{}},{"cell_type":"code","source":"rt_counts = df_train[\"rt\"].value_counts()\nplt.figure( figsize=(9,4))\nplt.pie(rt_counts, labels=[\"Has RT\", \"Has No RT\"], explode=[0.05,0], autopct='%1.1f%%')\nplt.title(\"Pie Chart of Percentage Tweets with 'RT' Vs Without\")\nplt.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:26.501592Z","iopub.execute_input":"2023-09-15T13:58:26.502214Z","iopub.status.idle":"2023-09-15T13:58:26.664130Z","shell.execute_reply.started":"2023-09-15T13:58:26.502169Z","shell.execute_reply":"2023-09-15T13:58:26.662600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the distribution of classes in our dataset we see that most tweets are Pro (display belief in) global warming and climate change (more than twice any other class). Other than that, alot of the tweets are News related. A fewer amount of the tweets are Neutral and the least amount of tweets are Anti (show little or no signs of belief in) global warming or climate change.","metadata":{}},{"cell_type":"code","source":"#Create a barplot for the train dataset classes\nsenti_counts = df_train[\"sentiment\"].value_counts()\nnews = senti_counts[2] \npro = senti_counts[1]   \nneutral = senti_counts[0]\nanti = senti_counts[-1]  \n\nplt.figure( figsize=(9,4))\nplt.barh(['News ','Pro','Neutral','Anti'], [news,pro,neutral,anti]) #Use matplotlib horizontal bar graph to compare classes of tweets.\nplt.xlabel('Count of Tweets') #X-label of the data\nplt.ylabel('Tweet Classification') #Y_label of the data \nplt.title('Distribution of Classes In The Dataset') #Give the data a title 'Dataset lables distribution'\nplt.show() ##Display the dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:26.666619Z","iopub.execute_input":"2023-09-15T13:58:26.667626Z","iopub.status.idle":"2023-09-15T13:58:26.954262Z","shell.execute_reply.started":"2023-09-15T13:58:26.667544Z","shell.execute_reply":"2023-09-15T13:58:26.953037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing The Overall Sentiment of The Data\n\nThe sentiment_score function is used to get a better idea of the what the underlying sentiment each classification has and to see if any if the classes correlate in term of it. The sentiment analyzer gives each tweet a score between 0 and 1 for each catagory of positive, negative and neutral. If tthe primary overall sentiment of a class is neutral then the function will output a secondary sentiment score to give the best overall picture of the general sentiment of each class.","metadata":{}},{"cell_type":"code","source":"def sentiment_score(df): # evaluates the sentiment of each tweet numerically\n    sentiments = SentimentIntensityAnalyzer()\n    df[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in df[\"message\"]]\n    df[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in df[\"message\"]]\n    df[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in df[\"message\"]]\n    a = sum(df[\"Positive\"])\n    b = sum(df[\"Negative\"])\n    c = sum(df[\"Neutral\"])\n    return (a, b, c) # outputs the overall score of the dataset per catagory\n\ndef senti_score_analyzer(score):  # analyzes the sentiment score catagorically\n    result =[]\n    a = score[0]\n    b = score[1]\n    c = score[2]\n\n    if (a>b) and (a>c):\n        result.append(\"Positive ðŸ˜Š\")\n    elif (b>a) and (b>c):\n        result.append(\"Negative ðŸ˜ \")\n    else:\n        result.append(\"Neutral ðŸ™‚\")\n    if result[0] == \"Neutral ðŸ™‚\":\n        if a > b:\n            result.append(\"Positive ðŸ˜Š\")\n        else:\n            result.append(\"Negative ðŸ˜ \")\n    return result # outputs the value as a list of catagories","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:26.961783Z","iopub.execute_input":"2023-09-15T13:58:26.962209Z","iopub.status.idle":"2023-09-15T13:58:26.977073Z","shell.execute_reply.started":"2023-09-15T13:58:26.962173Z","shell.execute_reply":"2023-09-15T13:58:26.975848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we apply the sentiment_score function to the dataset as a whole.","metadata":{}},{"cell_type":"code","source":"set_score = sentiment_score(df_train)\nset_sentiment = senti_score_analyzer(set_score)\n\nprint(\"The dataset is mostly\", set_sentiment[0], \"in sentiment\")\nprint(\"The dataset as a whole has an overall underlying sentiment of\", set_sentiment[-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:26.979000Z","iopub.execute_input":"2023-09-15T13:58:26.979656Z","iopub.status.idle":"2023-09-15T13:58:38.689058Z","shell.execute_reply.started":"2023-09-15T13:58:26.979621Z","shell.execute_reply":"2023-09-15T13:58:38.687840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news = df_train.copy()[df_train['sentiment'] == 2]\nnews_score = sentiment_score(df_news)\nnews_sentiment = senti_score_analyzer(news_score)\n\nprint(\"The news class is mostly\", news_sentiment[0], \"in sentiment\")\nprint(\"The news class has an overall underlying sentiment of\", news_sentiment[-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:38.690946Z","iopub.execute_input":"2023-09-15T13:58:38.691424Z","iopub.status.idle":"2023-09-15T13:58:41.163466Z","shell.execute_reply.started":"2023-09-15T13:58:38.691382Z","shell.execute_reply":"2023-09-15T13:58:41.162240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pro = df_train.copy()[df_train['sentiment'] == 1]\npro_score = sentiment_score(df_pro)\npro_sentiment = senti_score_analyzer(pro_score)\n\nprint(\"The pro class is mostly\", pro_sentiment[0], \"in sentiment\")\nprint(\"The pro class has an overall underlying sentiment of\", pro_sentiment[-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:41.164771Z","iopub.execute_input":"2023-09-15T13:58:41.165194Z","iopub.status.idle":"2023-09-15T13:58:47.739252Z","shell.execute_reply.started":"2023-09-15T13:58:41.165165Z","shell.execute_reply":"2023-09-15T13:58:47.738058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_neutral = df_train.copy()[df_train['sentiment'] == 0]\nneutral_score = sentiment_score(df_neutral)\nneutral_sentiment = senti_score_analyzer(neutral_score)\n\nprint(\"The neutral class is mostly\", neutral_sentiment[0], \"in sentiment\")\nprint(\"The neutral class has an overall underlying sentiment of\", neutral_sentiment[-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:47.741453Z","iopub.execute_input":"2023-09-15T13:58:47.742356Z","iopub.status.idle":"2023-09-15T13:58:49.376616Z","shell.execute_reply.started":"2023-09-15T13:58:47.742310Z","shell.execute_reply":"2023-09-15T13:58:49.375456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_anti = df_train.copy()[df_train['sentiment'] == -1]\nanti_score = sentiment_score(df_anti)\nanti_sentiment = senti_score_analyzer(anti_score)\n\nprint(\"The anti class is mostly\", anti_sentiment[0], \"in sentiment\")\nprint(\"The anti class has an overall underlying sentiment of\", anti_sentiment[-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:49.378265Z","iopub.execute_input":"2023-09-15T13:58:49.378648Z","iopub.status.idle":"2023-09-15T13:58:50.395476Z","shell.execute_reply.started":"2023-09-15T13:58:49.378617Z","shell.execute_reply":"2023-09-15T13:58:50.394348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although the entire dataset is mostly neutral in sentiment, it is slightly biased towards the negative side in all of the four classes. The neutral class is the only exception. It has an underlying overall sentiment of positive.","metadata":{}},{"cell_type":"code","source":"data = [set_score, pro_score, anti_score, neutral_score, news_score]  # List of tuples\ntitles = [\"All Tweets\", \"Pro\", \"Anti\", \"Neutral\", \"News\"]\n\n\n\naccumulated_data = np.zeros((3, len(data)))  # Initialize an array of zeros\n\nfor i, (a, b, c) in enumerate(data):\n    accumulated_data[:, i] = [a, b, c]\n\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(9,4))\n\n# Create a list of x-coordinates for the bars\nx = np.arange(len(data))\n\n# Plot the bars for each variable\nfor idx, variable in enumerate(('Positive', 'Negative', 'Neutral')):\n    ax.bar(x + idx * 0.2, accumulated_data[idx, :], width=0.2, label=variable)\n\n# Customize the plot\nax.set_xticks(x)\nax.set_xticklabels([f'{titles[i]}' for i in range(len(data))])\nax.set_xlabel('Data Sets')\nax.set_ylabel('Sentiment Score')\nax.set_title('Sentiment Score Comparison Across Different Classes')\nax.legend()\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:50.396979Z","iopub.execute_input":"2023-09-15T13:58:50.397437Z","iopub.status.idle":"2023-09-15T13:58:50.743260Z","shell.execute_reply.started":"2023-09-15T13:58:50.397406Z","shell.execute_reply":"2023-09-15T13:58:50.742096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_dict = {}\nwords = text.split(\" \")\nfor word in words:\n    if word != \" \" and word !=\"\":\n        if word not in freq_dict:\n            freq_dict[word] = 1\n        else:\n            freq_dict[word] += 1","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:50.745137Z","iopub.execute_input":"2023-09-15T13:58:50.746027Z","iopub.status.idle":"2023-09-15T13:58:50.876973Z","shell.execute_reply.started":"2023-09-15T13:58:50.745987Z","shell.execute_reply":"2023-09-15T13:58:50.876008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the dictionary by values and get the top 20 items\nsorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:30]\ntop_20_words = dict(sorted_freq_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:50.878160Z","iopub.execute_input":"2023-09-15T13:58:50.879138Z","iopub.status.idle":"2023-09-15T13:58:50.904233Z","shell.execute_reply.started":"2023-09-15T13:58:50.879101Z","shell.execute_reply":"2023-09-15T13:58:50.902877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the x-labels and values from the top 20 data\nx_labels = list(top_20_words.keys())\nvalues = list(top_20_words.values())\n\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(9,4))\n\n# Plot the data\nax.bar(x_labels, values)\n\n# Customize the plot\nax.set_xlabel('Frequency')\nax.set_ylabel('Word Count')\nax.set_title('Top 30 Most Used Words')\n\n# Rotate the x-labels if needed\nplt.xticks(rotation=90)\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:50.905942Z","iopub.execute_input":"2023-09-15T13:58:50.906576Z","iopub.status.idle":"2023-09-15T13:58:51.383117Z","shell.execute_reply.started":"2023-09-15T13:58:50.906535Z","shell.execute_reply":"2023-09-15T13:58:51.381819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words = sum([word for word in freq_dict.values()])\nprint(\"The dataset has\", total_words, \"unique words in total.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:51.384828Z","iopub.execute_input":"2023-09-15T13:58:51.385968Z","iopub.status.idle":"2023-09-15T13:58:51.393878Z","shell.execute_reply.started":"2023-09-15T13:58:51.385926Z","shell.execute_reply":"2023-09-15T13:58:51.392353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature EngineeringÂ¶","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import TreebankWordTokenizer\nfrom nltk import SnowballStemmer\n\ntokeniser = TreebankWordTokenizer()\ndf_train['tokens'] = df_train['message'].apply(tokeniser.tokenize)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:51.395882Z","iopub.execute_input":"2023-09-15T13:58:51.396252Z","iopub.status.idle":"2023-09-15T13:58:54.223986Z","shell.execute_reply.started":"2023-09-15T13:58:51.396219Z","shell.execute_reply":"2023-09-15T13:58:54.222827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bag_of_words_count(words, word_dict={}):\n    \"\"\" this function takes in a list of words and returns a dictionary \n        with each word as a key, and the value represents the number of \n        times that word appeared\"\"\"\n    for word in words:\n        if word in word_dict.keys():\n            word_dict[word] += 1\n        else:\n            word_dict[word] = 1\n    return word_dict","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:54.225672Z","iopub.execute_input":"2023-09-15T13:58:54.226050Z","iopub.status.idle":"2023-09-15T13:58:54.233088Z","shell.execute_reply.started":"2023-09-15T13:58:54.226016Z","shell.execute_reply":"2023-09-15T13:58:54.231677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type_labels = list(df_train['sentiment'].unique())\nsentiment = {}\nfor kind in type_labels:\n    df = df_train.groupby('sentiment')\n    sentiment[kind] = {}\n    for row in df.get_group(kind)['tokens']:\n        sentiment[kind] = bag_of_words_count(row, sentiment[kind])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:54.235116Z","iopub.execute_input":"2023-09-15T13:58:54.235600Z","iopub.status.idle":"2023-09-15T13:58:54.352979Z","shell.execute_reply.started":"2023-09-15T13:58:54.235554Z","shell.execute_reply":"2023-09-15T13:58:54.351924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = SnowballStemmer('english')\ndef tweet_stemmer(words, stemmer):\n    return [stemmer.stem(word) for word in words]","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:54.354411Z","iopub.execute_input":"2023-09-15T13:58:54.354741Z","iopub.status.idle":"2023-09-15T13:58:54.360148Z","shell.execute_reply.started":"2023-09-15T13:58:54.354713Z","shell.execute_reply":"2023-09-15T13:58:54.359261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['stem'] = df_train['tokens'].apply(tweet_stemmer, args=(stemmer, ))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:54.361445Z","iopub.execute_input":"2023-09-15T13:58:54.361781Z","iopub.status.idle":"2023-09-15T13:58:58.303710Z","shell.execute_reply.started":"2023-09-15T13:58:54.361754Z","shell.execute_reply":"2023-09-15T13:58:58.302530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist([word for word in freq_dict.values() if word < 10],bins=10)\nplt.ylabel(\"# of words\")\nplt.xlabel(\"word frequency\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.305220Z","iopub.execute_input":"2023-09-15T13:58:58.305638Z","iopub.status.idle":"2023-09-15T13:58:58.730481Z","shell.execute_reply.started":"2023-09-15T13:58:58.305604Z","shell.execute_reply":"2023-09-15T13:58:58.729344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len([v for v in freq_dict.values() if v >= 5])) #words that appear more than 10 times\noccurs_more_than_5_times = sum([v for v in freq_dict.values() if v >= 5]) # amount words of the total that account for the above words account for\nprint(occurs_more_than_5_times)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.731981Z","iopub.execute_input":"2023-09-15T13:58:58.732469Z","iopub.status.idle":"2023-09-15T13:58:58.743258Z","shell.execute_reply.started":"2023-09-15T13:58:58.732413Z","shell.execute_reply":"2023-09-15T13:58:58.742194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"occurs_more_than_5_times / total_words","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.744510Z","iopub.execute_input":"2023-09-15T13:58:58.744907Z","iopub.status.idle":"2023-09-15T13:58:58.762085Z","shell.execute_reply.started":"2023-09-15T13:58:58.744875Z","shell.execute_reply":"2023-09-15T13:58:58.760900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.763459Z","iopub.execute_input":"2023-09-15T13:58:58.763817Z","iopub.status.idle":"2023-09-15T13:58:58.789971Z","shell.execute_reply.started":"2023-09-15T13:58:58.763785Z","shell.execute_reply":"2023-09-15T13:58:58.788744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building Model","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import resample","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.791807Z","iopub.execute_input":"2023-09-15T13:58:58.792365Z","iopub.status.idle":"2023-09-15T13:58:58.800854Z","shell.execute_reply.started":"2023-09-15T13:58:58.792207Z","shell.execute_reply":"2023-09-15T13:58:58.799509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlen = 2000\nnews_resampled = resample(df_news, random_state=2,n_samples = rlen) # reproducible results\npro_resampled = resample(df_pro, random_state=2,n_samples = rlen) # reproducible results\nanti_resampled = resample(df_anti, random_state=2,n_samples = rlen) # reproducible results\nneutral_resampled = resample(df_neutral, random_state=2,n_samples = rlen) # reproducible results\n\nresampled = pd.concat([news_resampled,pro_resampled,anti_resampled,neutral_resampled])\nresampled['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.802577Z","iopub.execute_input":"2023-09-15T13:58:58.803481Z","iopub.status.idle":"2023-09-15T13:58:58.827620Z","shell.execute_reply.started":"2023-09-15T13:58:58.803425Z","shell.execute_reply":"2023-09-15T13:58:58.826711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['news', 'pro', 'anti', 'neutral']\nlen_unsampled = [news, pro, anti, neutral]\nresampled_len = [rlen, rlen, rlen, rlen]\nlabels = df_train['sentiment'].unique()\nplt.bar(labels,len_unsampled,color='grey')\nplt.bar(labels,resampled_len,color='orange')\nplt.xticks(classes)\nplt.ylabel(\"# of observations\")\nplt.legend(['original','resampled'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:58.828731Z","iopub.execute_input":"2023-09-15T13:58:58.829054Z","iopub.status.idle":"2023-09-15T13:58:59.282307Z","shell.execute_reply.started":"2023-09-15T13:58:58.829027Z","shell.execute_reply":"2023-09-15T13:58:59.280367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect = CountVectorizer(stop_words='english', \n                             min_df=5, \n                             max_df=2000, \n                             ngram_range=(2, 3))\nx=vect.fit_transform(df_train['message'])\ny=df_train['sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.284094Z","iopub.status.idle":"2023-09-15T13:58:59.285095Z","shell.execute_reply.started":"2023-09-15T13:58:59.284774Z","shell.execute_reply":"2023-09-15T13:58:59.284806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split your data into training and testing sets:\nX_train,X_test,y_train,y_test =train_test_split(x,y,random_state=26)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.286557Z","iopub.status.idle":"2023-09-15T13:58:59.287749Z","shell.execute_reply.started":"2023-09-15T13:58:59.287509Z","shell.execute_reply":"2023-09-15T13:58:59.287535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.289264Z","iopub.status.idle":"2023-09-15T13:58:59.290408Z","shell.execute_reply.started":"2023-09-15T13:58:59.290083Z","shell.execute_reply":"2023-09-15T13:58:59.290114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrfr = RandomForestRegressor()\nrfc = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrfc_random = RandomizedSearchCV(estimator = rfc, \n                               param_distributions = random_grid, \n                               n_iter = 100, cv = 3, verbose=2, \n                               random_state=42, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.292153Z","iopub.status.idle":"2023-09-15T13:58:59.292611Z","shell.execute_reply.started":"2023-09-15T13:58:59.292387Z","shell.execute_reply":"2023-09-15T13:58:59.292406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_random.fit(x, y)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.293788Z","iopub.status.idle":"2023-09-15T13:58:59.294177Z","shell.execute_reply.started":"2023-09-15T13:58:59.293990Z","shell.execute_reply":"2023-09-15T13:58:59.294008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.295308Z","iopub.status.idle":"2023-09-15T13:58:59.295701Z","shell.execute_reply.started":"2023-09-15T13:58:59.295511Z","shell.execute_reply":"2023-09-15T13:58:59.295529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.297062Z","iopub.status.idle":"2023-09-15T13:58:59.297518Z","shell.execute_reply.started":"2023-09-15T13:58:59.297301Z","shell.execute_reply":"2023-09-15T13:58:59.297332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_search.best_params_\nrfc_best_params = RandomForestClassifier(n_estimators=1800,\n min_samples_split= 2,\n min_samples_leaf= 2,\n max_features= 'auto',\n max_depth= None,\n bootstrap= True)\nrfc_best_params.fit(x,y)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.300646Z","iopub.status.idle":"2023-09-15T13:58:59.301111Z","shell.execute_reply.started":"2023-09-15T13:58:59.300892Z","shell.execute_reply":"2023-09-15T13:58:59.300915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=rfc_best_params.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.302694Z","iopub.status.idle":"2023-09-15T13:58:59.303143Z","shell.execute_reply.started":"2023-09-15T13:58:59.302936Z","shell.execute_reply":"2023-09-15T13:58:59.302956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\nf1 = f1_score(y_test, y_pred,average='macro',zero_division =1)\nprecision = precision_score(y_test, y_pred,average='macro')\nrecall = recall_score(y_test, y_pred,average='macro',zero_division=1)\nprint(f1)\nprint(precision)\nprint(recall)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.304668Z","iopub.status.idle":"2023-09-15T13:58:59.305659Z","shell.execute_reply.started":"2023-09-15T13:58:59.305349Z","shell.execute_reply":"2023-09-15T13:58:59.305382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame(df_test,y_pred)\n#submission = output.join(res)\noutput.to_csv('submission.csv',index =False)\nprint(output) ","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.307705Z","iopub.status.idle":"2023-09-15T13:58:59.309351Z","shell.execute_reply.started":"2023-09-15T13:58:59.309004Z","shell.execute_reply":"2023-09-15T13:58:59.309036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import SnowballStemmer\nstemmer = SnowballStemmer('english')\ndef tweet_stemmer(words, stemmer):\n    return [stemmer.stem(word) for word in words]","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.310883Z","iopub.status.idle":"2023-09-15T13:58:59.311513Z","shell.execute_reply.started":"2023-09-15T13:58:59.311191Z","shell.execute_reply":"2023-09-15T13:58:59.311221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.apply(tweet_stemmer, args=(stemmer))\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:58:59.313057Z","iopub.status.idle":"2023-09-15T13:58:59.313662Z","shell.execute_reply.started":"2023-09-15T13:58:59.313370Z","shell.execute_reply":"2023-09-15T13:58:59.313398Z"},"trusted":true},"execution_count":null,"outputs":[]}]}